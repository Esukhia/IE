{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"title":"In [1]:"},"outputs":[],"source":["!pip install comet_ml\n","!pip install kaggle\n","!pip install seqeval[gpu]\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [2]:"},"outputs":[],"source":["!pip uninstall tensorflow\n","!pip install tensorflow==1.14"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [3]:"},"outputs":[],"source":["import os\n","os.environ['KAGGLE_USERNAME'] = 'tenzinx'\n","os.environ['KAGGLE_KEY'] = '6ff29a25a39909de7e620d0428d83eac'"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [4]:"},"outputs":[],"source":["!kaggle datasets download tenzinx/tibetan-enumeration-for-ner"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [5]:"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/Esukhia/NER/master/ner_tagger/embeddings/bo_word2vec_v1?token=AD3KLUEFS7ORNGDC2I3FAIS5NTO2S\n","!mv bo_word2vec_v1\\?token\\=AD3KLUEFS7ORNGDC2I3FAIS5NTO2S bo_word2vec_v1\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [6]:"},"outputs":[],"source":["from comet_ml import Experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [7]:"},"outputs":[],"source":["model_name = 'Bi-LSTM_CRF_Word2Vec'\n","version = 1\n","exp_name = f'{model_name}_v{version}'"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [8]:"},"outputs":[],"source":["%%writefile .env\n","COMET_API_KEY=vIyyGJwVBzI3hxt2layIDbyye"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [9]:"},"outputs":[],"source":["exp = Experiment(project_name=\"ner-enumeration-model\", auto_output_logging='simple')\n","exp.set_name(exp_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [10]:"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [11]:"},"outputs":[],"source":["WV = KeyedVectors.load_word2vec_format('bo_word2vec_v1', binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [12]:"},"outputs":[],"source":["data = pd.read_csv(\"tibetan-enumeration-for-ner.zip\")\n","data = data.fillna(method=\"ffill\")\n","data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [13]:"},"outputs":[],"source":["words = list(set(data[\"word\"].values))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [14]:"},"outputs":[],"source":["tags = list(set(data[\"tag\"].values))\n","tags.append('PAD')\n","n_tags = len(tags)\n","print('# tags:', n_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [15]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [16]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [17]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [18]:"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [19]:"},"outputs":[],"source":["plt.hist([len(s) for s in sentences], bins=50)\n","plt.xlabel('Sentence Length')\n","plt.ylabel('No. Sentences')\n","exp.log_figure('Sentence length distribution', plt)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [20]:"},"outputs":[],"source":["max_len = 150\n","word2idx = {}\n","word2idx['<UNK>'] = len(WV.vocab)\n","word2idx['<PAD>'] = len(WV.vocab) + 1\n","for w in words:\n","    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n","    else: word2idx[w] = word2idx['<UNK>']\n","idx2word = {i: t for t, i in word2idx.items()}\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {i: t for t, i in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [21]:"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [22]:"},"outputs":[],"source":["X = [[word2idx[w[0]] for w in s] for s in sentences]\n","X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [23]:"},"outputs":[],"source":["y = [[tag2idx[w[1]] for w in s] for s in sentences]\n","y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n","y = np.array([to_categorical(i, num_classes=n_tags) for i in y])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [24]:"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [25]:"},"outputs":[],"source":["random_state = 45\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)\n","print('No. Training dataset:', X_train.shape[0])\n","print('No. Validation dataset:', X_valid.shape[0])\n","print('No. Test dataset:', X_test.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [26]:"},"outputs":[],"source":["from keras import backend as K\n","from keras.models import Model, Input\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from keras_contrib.layers import CRF\n","from keras_contrib.losses.crf_losses import crf_loss\n","from keras.callbacks import Callback\n","from seqeval.metrics import f1_score, classification_report, precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [27]:"},"outputs":[],"source":["params = {\n","    #dataset\n","    'n_train': X_train.shape[0],\n","    \n","    # sizes hps\n","    'vocab_size': len(WV.vocab) + 2, #<UNK> and <PAD>\n","    'max_len': max_len,\n","    'num_classes': n_tags,\n","    'embedding_size': 150,\n","    \n","    # models hps\n","    'optimizer': 'adam',\n","    'model_type': model_name,\n","    'lstm_layer_1_units': 100,\n","    'dense_layer_units': 50,\n","    'dense_layer_activation': 'relu',\n","    'dropout': 0.1,\n","    'recurrent_dropout': 0.1,\n","    \n","    # training hps\n","    'batch_size': 32,\n","    'epochs': 30\n","}\n","\n","exp.log_parameters(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [28]:"},"outputs":[],"source":["embedding_matrix = np.random.rand(params['vocab_size'], params['embedding_size']).astype('float32')\n","for word, i in word2idx.items():\n","    if word in WV.vocab:\n","        embedding_matrix[i] = WV[word]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [29]:"},"outputs":[],"source":["len(WV.vocab), embedding_matrix.shape, embedding_matrix.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [30]:"},"outputs":[],"source":["input = Input(shape=(params['max_len'],))\n","model = Embedding(input_dim=params['vocab_size'], output_dim=params['embedding_size'], input_length=params['max_len'],\n","                  mask_zero=False, weights=[embedding_matrix], trainable=False)(input)\n","model = Dropout(params['dropout'])(model)\n","model = Bidirectional(LSTM(units=params['lstm_layer_1_units'], return_sequences=True, recurrent_dropout=params['recurrent_dropout']))(model)\n","model = TimeDistributed(Dense(params['dense_layer_units'], activation=params['dense_layer_activation']))(model)\n","crf = CRF(params['num_classes'])  # CRF layer\n","out = crf(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [31]:"},"outputs":[],"source":["model = Model(input, out)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [32]:"},"outputs":[],"source":["model.compile(optimizer=params['optimizer'], loss=crf_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [33]:"},"outputs":[],"source":["class F1Metrics(Callback):\n","\n","    def __init__(self, id2label, pad_value=0, validation_data=None, digits=4):\n","        \"\"\"\n","        Args:\n","            id2label (dict): id to label mapping.\n","            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n","            pad_value (int): padding value.\n","            digits (int or None): number of digits in printed classification report\n","              (use None to print only F1 score without a report).\n","        \"\"\"\n","        super(F1Metrics, self).__init__()\n","        self.id2label = id2label\n","        self.pad_value = pad_value\n","        self.validation_data = validation_data\n","        self.digits = digits\n","        self.is_fit = validation_data is None\n","\n","    def convert_idx_to_name(self, y, array_indexes):\n","        \"\"\"Convert label index to name.\n","        Args:\n","            y (np.ndarray): label index 2d array.\n","            array_indexes (list): list of valid index arrays for each row.\n","        Returns:\n","            y: label name list.\n","        \"\"\"\n","        y = [[self.id2label[idx] for idx in row[row_indexes]] for\n","             row, row_indexes in zip(y, array_indexes)]\n","        return y\n","\n","    def predict(self, X, y):\n","        \"\"\"Predict sequences.\n","        Args:\n","            X (np.ndarray): input data.\n","            y (np.ndarray): tags.\n","        Returns:\n","            y_true: true sequences.\n","            y_pred: predicted sequences.\n","        \"\"\"\n","        y_pred = self.model.predict_on_batch(X)\n","\n","        # reduce dimension.\n","        y_true = np.argmax(y, -1)\n","        y_pred = np.argmax(y_pred, -1)\n","\n","        non_pad_indexes = [np.nonzero(y_true_row != self.pad_value)[0] for y_true_row in y_true]\n","\n","        y_true = self.convert_idx_to_name(y_true, non_pad_indexes)\n","        y_pred = self.convert_idx_to_name(y_pred, non_pad_indexes)\n","\n","        return y_true, y_pred\n","\n","    def score(self, y_true, y_pred):\n","        \"\"\"Calculate f1 score.\n","        Args:\n","            y_true (list): true sequences.\n","            y_pred (list): predicted sequences.\n","        Returns:\n","            score: f1 score.\n","        \"\"\"\n","        score = f1_score(y_true, y_pred)\n","        print(' - valid_f1: {:04.2f}'.format(score * 100))\n","        return score\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if self.is_fit:\n","            self.on_epoch_end_fit(epoch, logs)\n","        else:\n","            self.on_epoch_end_fit_generator(epoch, logs)\n","\n","    def on_epoch_end_fit(self, epoch, logs={}):\n","        X = self.validation_data[0]\n","        y = self.validation_data[1]\n","        y_true, y_pred = self.predict(X, y)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score\n","\n","    def on_epoch_end_fit_generator(self, epoch, logs={}):\n","        y_true = []\n","        y_pred = []\n","        for X, y in self.validation_data:\n","            y_true_batch, y_pred_batch = self.predict(X, y)\n","            y_true.extend(y_true_batch)\n","            y_pred.extend(y_pred_batch)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [34]:"},"outputs":[],"source":["f1_metrics = F1Metrics(idx2tag, tag2idx['PAD'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [35]:"},"outputs":[],"source":["    history = model.fit(X_train, y_train, \n","                        batch_size=params['batch_size'], \n","                        epochs=params['epochs'], \n","                        validation_data=(X_valid, y_valid),\n","                        callbacks=[f1_metrics],\n","                        verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [36]:"},"outputs":[],"source":["f1_metrics = F1Metrics(idx2tag, tag2idx['PAD'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [37]:"},"outputs":[],"source":["with exp.train():\n","    history = model.fit(X_train, y_train, \n","                        batch_size=params['batch_size'], \n","                        epochs=params['epochs'], \n","                        validation_data=(X_valid, y_valid),\n","                        callbacks=[f1_metrics],\n","                        verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [38]:"},"outputs":[],"source":["def to_char(x, y, pred):\n","    for x, y, p in zip([idx2word[x] for x in x], [idx2tag[x] for x in y], [idx2tag[x] for x in pred]):\n","        print(x, y, p)\n","\n","def evaluate(X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    \n","    # reduce dimension.\n","    y_true = np.argmax(y_test, -1)\n","    y_pred = np.argmax(y_pred, -1)\n","    \n","    #to_char(X_test[1], y_true[1], y_pred[1]) \n","    \n","    # remove PAD labels\n","    non_pad_indexes = [np.nonzero(y_true_row != tag2idx['PAD'])[0] for y_true_row in y_true]\n","    y_true = f1_metrics.convert_idx_to_name(y_true, non_pad_indexes)\n","    y_pred = f1_metrics.convert_idx_to_name(y_pred, non_pad_indexes)\n","    \n","    # compute f1 score\n","    f1 = f1_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    print(classification_report(y_true, y_pred))\n","    return f1, precision, recall, y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [39]:"},"outputs":[],"source":["with exp.test():\n","    f1, precision, recall, y_pred = evaluate(X_test, y_test)\n","    metrics = {\n","        'f1': '{:04.2f}'.format(f1 * 100),\n","        'precision': '{:04.2f}'.format(precision * 100),\n","        'recall': '{:04.2f}'.format(recall * 100),\n","    }\n","    exp.log_metrics(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [40]:"},"outputs":[],"source":["model_fn = f'{exp_name}.h5'\n","model.save(model_fn)\n","exp.log_asset(file_data=model_fn, file_name=model_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [41]:"},"outputs":[],"source":["def show(i):\n","    p = model.predict(np.array([X_test[i]]))\n","    p = np.argmax(p, axis=-1)\n","    true = np.argmax(y_test[i], -1)\n","    print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n","    print(30 * \"=\")\n","    for w, t, pred in zip(X_test[i], true, p[0]):\n","        if idx2word[w] != '<PAD>':\n","            print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [42]:"},"outputs":[],"source":["show(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [43]:"},"outputs":[],"source":["show(32)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [44]:"},"outputs":[],"source":["show(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [45]:"},"outputs":[],"source":["exp.end()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [46]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(data):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if lemma:\n","            yield preprocess(lemma)\n","        else:\n","            yield preprocess(word)   "]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [47]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [48]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [49]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if lemma:\n","            yield preprocess(lemma)\n","        else:\n","            yield preprocess(word)   "]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [50]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [51]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [52]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [53]:"},"outputs":[],"source":["sentences[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [54]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [55]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [56]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [57]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [58]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [59]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if lemma:\n","            token = preprocess(lemma)\n","        else:\n","            token = preprocess(word)\n","        yield token   "]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [60]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [61]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [62]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [63]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [64]:"},"outputs":[],"source":["data.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [65]:"},"outputs":[],"source":["data = pd.read_csv(\"tibetan-enumeration-for-ner.zip\")\n","data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [66]:"},"outputs":[],"source":["data.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [67]:"},"outputs":[],"source":["words = list(set(data[\"word\"].values))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [68]:"},"outputs":[],"source":["tags = list(set(data[\"tag\"].values))\n","tags.append('PAD')\n","n_tags = len(tags)\n","print('# tags:', n_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [69]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if lemma:\n","            token = preprocess(lemma)\n","        else:\n","            token = preprocess(word)\n","        yield token   "]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [70]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [71]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [72]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [73]:"},"outputs":[],"source":["data[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [74]:"},"outputs":[],"source":["data.iloc(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [75]:"},"outputs":[],"source":["data.loc[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [76]:"},"outputs":[],"source":["data.loc[1]['lemma']"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [77]:"},"outputs":[],"source":["if data.loc[1]['lemma']:\n","  print('nan is true')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [78]:"},"outputs":[],"source":["if np.isnan(data.loc[1]['lemma']):\n","  print('nan is true')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [79]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if np.isnan(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [80]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [81]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [82]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if pd.isnull(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [83]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [84]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [85]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [86]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [87]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist()) if '?' not in w]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [88]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [89]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [90]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [91]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())] #if '?' not in w]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [92]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [93]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [94]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [95]:"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/Esukhia/NER/master/ner_tagger/embeddings/bo_word2vec_lammatized\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [96]:"},"outputs":[],"source":["WV = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [97]:"},"outputs":[],"source":["model_name = 'Bi-LSTM_CRF_Word2Vec_lammatized'\n","version = 1\n","exp_name = f'{model_name}_v{version}'"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [98]:"},"outputs":[],"source":["%%writefile .env\n","COMET_API_KEY=vIyyGJwVBzI3hxt2layIDbyye"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [99]:"},"outputs":[],"source":["exp = Experiment(project_name=\"ner-enumeration-model\", auto_output_logging='simple')\n","exp.set_name(exp_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [100]:"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [101]:"},"outputs":[],"source":["WV = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [102]:"},"outputs":[],"source":["data = pd.read_csv(\"tibetan-enumeration-for-ner.zip\")\n","data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [103]:"},"outputs":[],"source":["if np.isnan(data.loc[1]['lemma']):\n","  print('nan is true')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [104]:"},"outputs":[],"source":["words = list(set(data[\"word\"].values))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [105]:"},"outputs":[],"source":["tags = list(set(data[\"tag\"].values))\n","tags.append('PAD')\n","n_tags = len(tags)\n","print('# tags:', n_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [106]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(s):\n","    for word, lemma in zip(s[\"word\"].values.tolist(), s[\"lemma\"].values.tolist()):\n","        if pd.isnull(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [107]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())] #if '?' not in w]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [108]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [109]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [110]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [111]:"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [112]:"},"outputs":[],"source":["plt.hist([len(s) for s in sentences], bins=50)\n","plt.xlabel('Sentence Length')\n","plt.ylabel('No. Sentences')\n","exp.log_figure('Sentence length distribution', plt)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [113]:"},"outputs":[],"source":["max_len = 150\n","word2idx = {}\n","word2idx['<UNK>'] = len(WV.vocab)\n","word2idx['<PAD>'] = len(WV.vocab) + 1\n","for w in words:\n","    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n","    else: word2idx[w] = word2idx['<UNK>']\n","idx2word = {i: t for t, i in word2idx.items()}\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {i: t for t, i in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [114]:"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [115]:"},"outputs":[],"source":["X = [[word2idx[w[0]] for w in s] for s in sentences]\n","X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [116]:"},"outputs":[],"source":["word2idx['??????']"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [117]:"},"outputs":[],"source":["word2idx['???????']"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [118]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(df):\n","    for word, lemma in zip(df[\"word\"].values.tolist(), df[\"lemma\"].values.tolist()):\n","        if pd.isnull(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [119]:"},"outputs":[],"source":["words = list(set(get_lemmatized_tokens(data)))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [120]:"},"outputs":[],"source":["exp.end()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [121]:"},"outputs":[],"source":["max_len = 150\n","word2idx = {}\n","word2idx['<UNK>'] = len(WV.vocab)\n","word2idx['<PAD>'] = len(WV.vocab) + 1\n","for w in words:\n","    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n","    else: word2idx[w] = word2idx['<UNK>']\n","idx2word = {i: t for t, i in word2idx.items()}\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {i: t for t, i in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [122]:"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [123]:"},"outputs":[],"source":["X = [[word2idx[w[0]] for w in s] for s in sentences]\n","X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [124]:"},"outputs":[],"source":["model_name = 'Bi-LSTM_CRF_Word2Vec_lammatized'\n","version = 1\n","exp_name = f'{model_name}_v{version}'"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [125]:"},"outputs":[],"source":["%%writefile .env\n","COMET_API_KEY=vIyyGJwVBzI3hxt2layIDbyye"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [126]:"},"outputs":[],"source":["exp = Experiment(project_name=\"ner-enumeration-model\", auto_output_logging='simple')\n","exp.set_name(exp_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [127]:"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [128]:"},"outputs":[],"source":["WV = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [129]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(df):\n","    for word, lemma in zip(df[\"word\"].values.tolist(), df[\"lemma\"].values.tolist()):\n","        if pd.isnull(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [130]:"},"outputs":[],"source":["data = pd.read_csv(\"tibetan-enumeration-for-ner.zip\")\n","data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [131]:"},"outputs":[],"source":["words = list(set(get_lemmatized_tokens(data)))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [132]:"},"outputs":[],"source":["tags = list(set(data[\"tag\"].values))\n","tags.append('PAD')\n","n_tags = len(tags)\n","print('# tags:', n_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [133]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())] #if '?' not in w]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [134]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [135]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [136]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [137]:"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [138]:"},"outputs":[],"source":["plt.hist([len(s) for s in sentences], bins=50)\n","plt.xlabel('Sentence Length')\n","plt.ylabel('No. Sentences')\n","exp.log_figure('Sentence length distribution', plt)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [139]:"},"outputs":[],"source":["max_len = 150\n","word2idx = {}\n","word2idx['<UNK>'] = len(WV.vocab)\n","word2idx['<PAD>'] = len(WV.vocab) + 1\n","for w in words:\n","    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n","    else: word2idx[w] = word2idx['<UNK>']\n","idx2word = {i: t for t, i in word2idx.items()}\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {i: t for t, i in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [140]:"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [141]:"},"outputs":[],"source":["X = [[word2idx[w[0]] for w in s] for s in sentences]\n","X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [142]:"},"outputs":[],"source":["y = [[tag2idx[w[1]] for w in s] for s in sentences]\n","y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n","y = np.array([to_categorical(i, num_classes=n_tags) for i in y])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [143]:"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [144]:"},"outputs":[],"source":["random_state = 45\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)\n","print('No. Training dataset:', X_train.shape[0])\n","print('No. Validation dataset:', X_valid.shape[0])\n","print('No. Test dataset:', X_test.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [145]:"},"outputs":[],"source":["from keras import backend as K\n","from keras.models import Model, Input\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from keras_contrib.layers import CRF\n","from keras_contrib.losses.crf_losses import crf_loss\n","from keras.callbacks import Callback\n","from seqeval.metrics import f1_score, classification_report, precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [146]:"},"outputs":[],"source":["params = {\n","    #dataset\n","    'n_train': X_train.shape[0],\n","    \n","    # sizes hps\n","    'vocab_size': len(WV.vocab) + 2, #<UNK> and <PAD>\n","    'max_len': max_len,\n","    'num_classes': n_tags,\n","    'embedding_size': 150,\n","    \n","    # models hps\n","    'optimizer': 'adam',\n","    'model_type': model_name,\n","    'lstm_layer_1_units': 100,\n","    'dense_layer_units': 50,\n","    'dense_layer_activation': 'relu',\n","    'dropout': 0.1,\n","    'recurrent_dropout': 0.1,\n","    \n","    # training hps\n","    'batch_size': 32,\n","    'epochs': 30\n","}\n","\n","exp.log_parameters(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [147]:"},"outputs":[],"source":["embedding_matrix = np.random.rand(params['vocab_size'], params['embedding_size']).astype('float32')\n","for word, i in word2idx.items():\n","    if word in WV.vocab:\n","        embedding_matrix[i] = WV[word]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [148]:"},"outputs":[],"source":["len(WV.vocab), embedding_matrix.shape, embedding_matrix.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [149]:"},"outputs":[],"source":["input = Input(shape=(params['max_len'],))\n","model = Embedding(input_dim=params['vocab_size'], output_dim=params['embedding_size'], input_length=params['max_len'],\n","                  mask_zero=False, weights=[embedding_matrix], trainable=False)(input)\n","model = Dropout(params['dropout'])(model)\n","model = Bidirectional(LSTM(units=params['lstm_layer_1_units'], return_sequences=True, recurrent_dropout=params['recurrent_dropout']))(model)\n","model = TimeDistributed(Dense(params['dense_layer_units'], activation=params['dense_layer_activation']))(model)\n","crf = CRF(params['num_classes'])  # CRF layer\n","out = crf(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [150]:"},"outputs":[],"source":["model = Model(input, out)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [151]:"},"outputs":[],"source":["model.compile(optimizer=params['optimizer'], loss=crf_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [152]:"},"outputs":[],"source":["class F1Metrics(Callback):\n","\n","    def __init__(self, id2label, pad_value=0, validation_data=None, digits=4):\n","        \"\"\"\n","        Args:\n","            id2label (dict): id to label mapping.\n","            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n","            pad_value (int): padding value.\n","            digits (int or None): number of digits in printed classification report\n","              (use None to print only F1 score without a report).\n","        \"\"\"\n","        super(F1Metrics, self).__init__()\n","        self.id2label = id2label\n","        self.pad_value = pad_value\n","        self.validation_data = validation_data\n","        self.digits = digits\n","        self.is_fit = validation_data is None\n","\n","    def convert_idx_to_name(self, y, array_indexes):\n","        \"\"\"Convert label index to name.\n","        Args:\n","            y (np.ndarray): label index 2d array.\n","            array_indexes (list): list of valid index arrays for each row.\n","        Returns:\n","            y: label name list.\n","        \"\"\"\n","        y = [[self.id2label[idx] for idx in row[row_indexes]] for\n","             row, row_indexes in zip(y, array_indexes)]\n","        return y\n","\n","    def predict(self, X, y):\n","        \"\"\"Predict sequences.\n","        Args:\n","            X (np.ndarray): input data.\n","            y (np.ndarray): tags.\n","        Returns:\n","            y_true: true sequences.\n","            y_pred: predicted sequences.\n","        \"\"\"\n","        y_pred = self.model.predict_on_batch(X)\n","\n","        # reduce dimension.\n","        y_true = np.argmax(y, -1)\n","        y_pred = np.argmax(y_pred, -1)\n","\n","        non_pad_indexes = [np.nonzero(y_true_row != self.pad_value)[0] for y_true_row in y_true]\n","\n","        y_true = self.convert_idx_to_name(y_true, non_pad_indexes)\n","        y_pred = self.convert_idx_to_name(y_pred, non_pad_indexes)\n","\n","        return y_true, y_pred\n","\n","    def score(self, y_true, y_pred):\n","        \"\"\"Calculate f1 score.\n","        Args:\n","            y_true (list): true sequences.\n","            y_pred (list): predicted sequences.\n","        Returns:\n","            score: f1 score.\n","        \"\"\"\n","        score = f1_score(y_true, y_pred)\n","        print(' - valid_f1: {:04.2f}'.format(score * 100))\n","        return score\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if self.is_fit:\n","            self.on_epoch_end_fit(epoch, logs)\n","        else:\n","            self.on_epoch_end_fit_generator(epoch, logs)\n","\n","    def on_epoch_end_fit(self, epoch, logs={}):\n","        X = self.validation_data[0]\n","        y = self.validation_data[1]\n","        y_true, y_pred = self.predict(X, y)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score\n","\n","    def on_epoch_end_fit_generator(self, epoch, logs={}):\n","        y_true = []\n","        y_pred = []\n","        for X, y in self.validation_data:\n","            y_true_batch, y_pred_batch = self.predict(X, y)\n","            y_true.extend(y_true_batch)\n","            y_pred.extend(y_pred_batch)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [153]:"},"outputs":[],"source":["f1_metrics = F1Metrics(idx2tag, tag2idx['PAD'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [154]:"},"outputs":[],"source":["with exp.train():\n","    history = model.fit(X_train, y_train, \n","                        batch_size=params['batch_size'], \n","                        epochs=params['epochs'], \n","                        validation_data=(X_valid, y_valid),\n","                        callbacks=[f1_metrics],\n","                        verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [155]:"},"outputs":[],"source":["def to_char(x, y, pred):\n","    for x, y, p in zip([idx2word[x] for x in x], [idx2tag[x] for x in y], [idx2tag[x] for x in pred]):\n","        print(x, y, p)\n","\n","def evaluate(X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    \n","    # reduce dimension.\n","    y_true = np.argmax(y_test, -1)\n","    y_pred = np.argmax(y_pred, -1)\n","    \n","    #to_char(X_test[1], y_true[1], y_pred[1]) \n","    \n","    # remove PAD labels\n","    non_pad_indexes = [np.nonzero(y_true_row != tag2idx['PAD'])[0] for y_true_row in y_true]\n","    y_true = f1_metrics.convert_idx_to_name(y_true, non_pad_indexes)\n","    y_pred = f1_metrics.convert_idx_to_name(y_pred, non_pad_indexes)\n","    \n","    # compute f1 score\n","    f1 = f1_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    print(classification_report(y_true, y_pred))\n","    return f1, precision, recall, y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [156]:"},"outputs":[],"source":["with exp.test():\n","    f1, precision, recall, y_pred = evaluate(X_test, y_test)\n","    metrics = {\n","        'f1': '{:04.2f}'.format(f1 * 100),\n","        'precision': '{:04.2f}'.format(precision * 100),\n","        'recall': '{:04.2f}'.format(recall * 100),\n","    }\n","    exp.log_metrics(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [157]:"},"outputs":[],"source":["model_fn = f'{exp_name}.h5'\n","model.save(model_fn)\n","exp.log_asset(file_data=model_fn, file_name=model_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [158]:"},"outputs":[],"source":["def show(i):\n","    p = model.predict(np.array([X_test[i]]))\n","    p = np.argmax(p, axis=-1)\n","    true = np.argmax(y_test[i], -1)\n","    print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n","    print(30 * \"=\")\n","    for w, t, pred in zip(X_test[i], true, p[0]):\n","        if idx2word[w] != '<PAD>':\n","            print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [159]:"},"outputs":[],"source":["show(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [160]:"},"outputs":[],"source":["show(32)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [161]:"},"outputs":[],"source":["show(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [162]:"},"outputs":[],"source":["exp.end()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [163]:"},"outputs":[],"source":["model_name = 'Bi-LSTM_CRF_Word2Vec_lammatized'\n","version = 2\n","exp_name = f'{model_name}_v{version}'"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [164]:"},"outputs":[],"source":["%%writefile .env\n","COMET_API_KEY=vIyyGJwVBzI3hxt2layIDbyye"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [165]:"},"outputs":[],"source":["exp = Experiment(project_name=\"ner-enumeration-model\", auto_output_logging='simple')\n","exp.set_name(exp_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [166]:"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [167]:"},"outputs":[],"source":["WV = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [168]:"},"outputs":[],"source":["def preprocess(token):\n","    token = token.replace(' ', '_')\n","    return token\n","\n","def get_lemmatized_tokens(df):\n","    for word, lemma in zip(df[\"word\"].values.tolist(), df[\"lemma\"].values.tolist()):\n","        if pd.isnull(lemma):\n","            yield preprocess(word)\n","        else:\n","            yield preprocess(lemma)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [169]:"},"outputs":[],"source":["data = pd.read_csv(\"tibetan-enumeration-for-ner.zip\")\n","data.tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [170]:"},"outputs":[],"source":["words = list(set(get_lemmatized_tokens(data)))\n","n_words = len(words)\n","print(\"# words:\", n_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [171]:"},"outputs":[],"source":["tags = list(set(data[\"tag\"].values))\n","tags.append('PAD')\n","n_tags = len(tags)\n","print('# tags:', n_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [172]:"},"outputs":[],"source":["class SentenceGetter(object):\n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, t) for w, t in zip(get_lemmatized_tokens(s), s[\"tag\"].values.tolist())] #if '?' not in w]\n","        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","        \n","    def get_next(self):\n","        try:\n","            s = self.grouped[self.n_sent]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [173]:"},"outputs":[],"source":["getter = SentenceGetter(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [174]:"},"outputs":[],"source":["sentences = getter.sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [175]:"},"outputs":[],"source":["sentences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [176]:"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [177]:"},"outputs":[],"source":["plt.hist([len(s) for s in sentences], bins=50)\n","plt.xlabel('Sentence Length')\n","plt.ylabel('No. Sentences')\n","exp.log_figure('Sentence length distribution', plt)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [178]:"},"outputs":[],"source":["max_len = 150\n","word2idx = {}\n","word2idx['<UNK>'] = len(WV.vocab)\n","word2idx['<PAD>'] = len(WV.vocab) + 1\n","for w in words:\n","    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n","    else: word2idx[w] = word2idx['<UNK>']\n","idx2word = {i: t for t, i in word2idx.items()}\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {i: t for t, i in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [179]:"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [180]:"},"outputs":[],"source":["X = [[word2idx[w[0]] for w in s] for s in sentences]\n","X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [181]:"},"outputs":[],"source":["y = [[tag2idx[w[1]] for w in s] for s in sentences]\n","y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n","y = np.array([to_categorical(i, num_classes=n_tags) for i in y])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [182]:"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [183]:"},"outputs":[],"source":["random_state = 45\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)\n","print('No. Training dataset:', X_train.shape[0])\n","print('No. Validation dataset:', X_valid.shape[0])\n","print('No. Test dataset:', X_test.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [184]:"},"outputs":[],"source":["from keras import backend as K\n","from keras.models import Model, Input\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from keras_contrib.layers import CRF\n","from keras_contrib.losses.crf_losses import crf_loss\n","from keras.callbacks import Callback\n","from seqeval.metrics import f1_score, classification_report, precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [185]:"},"outputs":[],"source":["params = {\n","    #dataset\n","    'n_train': X_train.shape[0],\n","    \n","    # sizes hps\n","    'vocab_size': len(WV.vocab) + 2, #<UNK> and <PAD>\n","    'max_len': max_len,\n","    'num_classes': n_tags,\n","    'embedding_size': 150,\n","    \n","    # models hps\n","    'optimizer': 'adam',\n","    'model_type': model_name,\n","    'lstm_layer_1_units': 100,\n","    'dense_layer_units': 50,\n","    'dense_layer_activation': 'relu',\n","    'dropout': 0.1,\n","    'recurrent_dropout': 0.1,\n","    \n","    # training hps\n","    'batch_size': 24,\n","    'epochs': 40\n","}\n","\n","exp.log_parameters(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [186]:"},"outputs":[],"source":["embedding_matrix = np.random.rand(params['vocab_size'], params['embedding_size']).astype('float32')\n","for word, i in word2idx.items():\n","    if word in WV.vocab:\n","        embedding_matrix[i] = WV[word]"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [187]:"},"outputs":[],"source":["len(WV.vocab), embedding_matrix.shape, embedding_matrix.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [188]:"},"outputs":[],"source":["input = Input(shape=(params['max_len'],))\n","model = Embedding(input_dim=params['vocab_size'], output_dim=params['embedding_size'], input_length=params['max_len'],\n","                  mask_zero=False, weights=[embedding_matrix], trainable=False)(input)\n","model = Dropout(params['dropout'])(model)\n","model = Bidirectional(LSTM(units=params['lstm_layer_1_units'], return_sequences=True, recurrent_dropout=params['recurrent_dropout']))(model)\n","model = TimeDistributed(Dense(params['dense_layer_units'], activation=params['dense_layer_activation']))(model)\n","crf = CRF(params['num_classes'])  # CRF layer\n","out = crf(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [189]:"},"outputs":[],"source":["model = Model(input, out)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [190]:"},"outputs":[],"source":["model.compile(optimizer=params['optimizer'], loss=crf_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [191]:"},"outputs":[],"source":["class F1Metrics(Callback):\n","\n","    def __init__(self, id2label, pad_value=0, validation_data=None, digits=4):\n","        \"\"\"\n","        Args:\n","            id2label (dict): id to label mapping.\n","            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n","            pad_value (int): padding value.\n","            digits (int or None): number of digits in printed classification report\n","              (use None to print only F1 score without a report).\n","        \"\"\"\n","        super(F1Metrics, self).__init__()\n","        self.id2label = id2label\n","        self.pad_value = pad_value\n","        self.validation_data = validation_data\n","        self.digits = digits\n","        self.is_fit = validation_data is None\n","\n","    def convert_idx_to_name(self, y, array_indexes):\n","        \"\"\"Convert label index to name.\n","        Args:\n","            y (np.ndarray): label index 2d array.\n","            array_indexes (list): list of valid index arrays for each row.\n","        Returns:\n","            y: label name list.\n","        \"\"\"\n","        y = [[self.id2label[idx] for idx in row[row_indexes]] for\n","             row, row_indexes in zip(y, array_indexes)]\n","        return y\n","\n","    def predict(self, X, y):\n","        \"\"\"Predict sequences.\n","        Args:\n","            X (np.ndarray): input data.\n","            y (np.ndarray): tags.\n","        Returns:\n","            y_true: true sequences.\n","            y_pred: predicted sequences.\n","        \"\"\"\n","        y_pred = self.model.predict_on_batch(X)\n","\n","        # reduce dimension.\n","        y_true = np.argmax(y, -1)\n","        y_pred = np.argmax(y_pred, -1)\n","\n","        non_pad_indexes = [np.nonzero(y_true_row != self.pad_value)[0] for y_true_row in y_true]\n","\n","        y_true = self.convert_idx_to_name(y_true, non_pad_indexes)\n","        y_pred = self.convert_idx_to_name(y_pred, non_pad_indexes)\n","\n","        return y_true, y_pred\n","\n","    def score(self, y_true, y_pred):\n","        \"\"\"Calculate f1 score.\n","        Args:\n","            y_true (list): true sequences.\n","            y_pred (list): predicted sequences.\n","        Returns:\n","            score: f1 score.\n","        \"\"\"\n","        score = f1_score(y_true, y_pred)\n","        print(' - valid_f1: {:04.2f}'.format(score * 100))\n","        return score\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if self.is_fit:\n","            self.on_epoch_end_fit(epoch, logs)\n","        else:\n","            self.on_epoch_end_fit_generator(epoch, logs)\n","\n","    def on_epoch_end_fit(self, epoch, logs={}):\n","        X = self.validation_data[0]\n","        y = self.validation_data[1]\n","        y_true, y_pred = self.predict(X, y)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score\n","\n","    def on_epoch_end_fit_generator(self, epoch, logs={}):\n","        y_true = []\n","        y_pred = []\n","        for X, y in self.validation_data:\n","            y_true_batch, y_pred_batch = self.predict(X, y)\n","            y_true.extend(y_true_batch)\n","            y_pred.extend(y_pred_batch)\n","        score = self.score(y_true, y_pred)\n","        logs['valid_f1'] = score"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [192]:"},"outputs":[],"source":["f1_metrics = F1Metrics(idx2tag, tag2idx['PAD'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [193]:"},"outputs":[],"source":["with exp.train():\n","    history = model.fit(X_train, y_train, \n","                        batch_size=params['batch_size'], \n","                        epochs=params['epochs'], \n","                        validation_data=(X_valid, y_valid),\n","                        callbacks=[f1_metrics],\n","                        verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [194]:"},"outputs":[],"source":["def to_char(x, y, pred):\n","    for x, y, p in zip([idx2word[x] for x in x], [idx2tag[x] for x in y], [idx2tag[x] for x in pred]):\n","        print(x, y, p)\n","\n","def evaluate(X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    \n","    # reduce dimension.\n","    y_true = np.argmax(y_test, -1)\n","    y_pred = np.argmax(y_pred, -1)\n","    \n","    #to_char(X_test[1], y_true[1], y_pred[1]) \n","    \n","    # remove PAD labels\n","    non_pad_indexes = [np.nonzero(y_true_row != tag2idx['PAD'])[0] for y_true_row in y_true]\n","    y_true = f1_metrics.convert_idx_to_name(y_true, non_pad_indexes)\n","    y_pred = f1_metrics.convert_idx_to_name(y_pred, non_pad_indexes)\n","    \n","    # compute f1 score\n","    f1 = f1_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    print(classification_report(y_true, y_pred))\n","    return f1, precision, recall, y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [195]:"},"outputs":[],"source":["with exp.test():\n","    f1, precision, recall, y_pred = evaluate(X_test, y_test)\n","    metrics = {\n","        'f1': '{:04.2f}'.format(f1 * 100),\n","        'precision': '{:04.2f}'.format(precision * 100),\n","        'recall': '{:04.2f}'.format(recall * 100),\n","    }\n","    exp.log_metrics(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [196]:"},"outputs":[],"source":["model_fn = f'{exp_name}.h5'\n","model.save(model_fn)\n","exp.log_asset(file_data=model_fn, file_name=model_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [197]:"},"outputs":[],"source":["def show(i):\n","    p = model.predict(np.array([X_test[i]]))\n","    p = np.argmax(p, axis=-1)\n","    true = np.argmax(y_test[i], -1)\n","    print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n","    print(30 * \"=\")\n","    for w, t, pred in zip(X_test[i], true, p[0]):\n","        if idx2word[w] != '<PAD>':\n","            print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [198]:"},"outputs":[],"source":["show(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [199]:"},"outputs":[],"source":["show(32)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [200]:"},"outputs":[],"source":["show(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2,"title":"In [201]:"},"outputs":[],"source":["exp.end()"]}],"metadata":{"jupytext":{"main_language":"python","text_representation":{"extension":".py","format_name":"percent"}}},"nbformat":4,"nbformat_minor":2}